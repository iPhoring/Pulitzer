{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulitzer Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Objective:\n",
    "The goal is to find a relationship between Pulitzer to GDP, Crime, and Population. The dataset is provided by the various organization as listed in the Source and Credits section. \n",
    "\n",
    "I would also like to check if:\n",
    "Q1. Newspaper with the maximum number of Pulitzer prices? \n",
    "Q2. What are the top 5 states\n",
    "Q3. To find if there is any Correlation between Crime, GDP, and Population on Pulitzer? For example - higher GDP means more prices (confounding parameter could be more journalists) or crime-prone cities incubate investigative journalism resulting in more Pulitzer.\n",
    "Q4. If there is any correlation between daily circulation and crime rate?\n",
    "Q5. To find socioeconomic factors affecting Pulitzer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Data Source and Credits:\n",
    "Raw Data Source and Credits:\n",
    "Pulitzer Data is available FiveThirtyEight GIT HUB site <br>\n",
    "https://github.com/fivethirtyeight/data/blob/master/pulitzer/pulitzer-circulation-data.csv \n",
    "\n",
    "\n",
    "Crime Data by State and US: <br>\n",
    "http://www.usa.com/rank/us--crime-index--state-rank.htm <br>\n",
    "https://ucr.fbi.gov/crime-in-the-u.s/2014/crime-in-the-u.s.-2014/tables/table-1 <br>\n",
    "\n",
    "GDP for US and States: <br>\n",
    "https://www.usgovernmentrevenue.com/download_multi_year_2000_2014USb_19c1li101mcn_F1cF0t <br>\n",
    "\n",
    "Population by State and US: <br>\n",
    "https://www.census.gov/data/datasets/2016/demo/popest/state-total.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Prerequisites and Environment:\n",
    "1. Jupyter server running in Google Cloud Platform (\"GCP\")\n",
    "2. Spark slaves running on GCP\n",
    "3. Cassandra cluster running on GCP\n",
    "4. Spark master and jupyter server running on same VM.\n",
    "\n",
    "**Please see my blog for detail setup.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Approach:\n",
    "The solution is specifically designed to achieve a controlled and structured approach to minimize data quality, relationship and format issues that may be present. <br>\n",
    "The solution is sub-divided into three phases: <br>\n",
    "\n",
    "### 4.1 Data Assembly - Phase I:\n",
    "This phase of the project is designed to gather and do basic cleanup like join, merge, add or update attributes.\n",
    "\n",
    "### 4.2 Data Preparation - Phase II: \n",
    "This phase of the project is designed to merge data collected from a persistent and reproducible source. At this stage, the data is clean and identifiable but has not gone thru any required transformation.\n",
    "\n",
    "### 4.3 Analysis and Discovery - Phase III: \n",
    "This phase of the project is designed to validate and explore the dataset for all the problems listed in the “Objective” section of this notebook.\n",
    "\n",
    "Please note that the above steps are NOT sequential. I plan to take an iterative approach to improve the dataset accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Data Preparation - Phase I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Gathering Pulitzer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FiveThirtyEight.com is the source of the Pulitzer raw data(\"Pulitzer\"). US State, GDP, population or crime data are not mapped to Pulitzer. To map these other dimensions we 1st need to assign the raw Pulitzer data with a US State. This activity is done manually based on the name of the newspaper.\n",
    "\n",
    "Step 1: Adding US state mapping to newspaper data. This step is done manually due to lack of common source, i.e., mapping newspaper to state not available. <br>\n",
    "Step 2: Upload the file to GCP bucket so that it can be accessed from the Jupyter server which is also running in GCP. <br>\n",
    "Step 2(Optional): Please check my blog for how to setup Jupyter, Spark and Cassandra cluster in GCP. <br>\n",
    "\n",
    "### Assumption: For national or international newspaper I have used the HO state as the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1.1 Read and Prepare Pulitzer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "import uuid\n",
    "\n",
    "#Reading Google Buckets for files\n",
    "client = storage.Client()\n",
    "bucket=client.get_bucket('capstone_project_sr')\n",
    "blob = storage.Blob('pulitzer.csv',bucket)\n",
    "with open('pulitzer.csv', 'wb') as file_obj:\n",
    "    blob.download_to_file(file_obj)\n",
    "df=pd.read_csv('pulitzer.csv',sep=',',header=0, \\\n",
    "               names=['Newspaper','state','DailyCirculation_2004',\\\n",
    "               'DailyCirculation_2013',\\\n",
    "               'ChangeInDailyCirculation_2004_2013',\\\n",
    "               'WinnersAndFinalists_1990_2003',\\\n",
    "               'WinnersAndFinalists_2004_2014',\\\n",
    "               'WinnersAndFinalists_1990_2014'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding basic data audit fields just in case we run into data conflicts in later phases of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tz = timezone('EST') # adding time zone info\n",
    "datetime.now(tz) \n",
    "df['Entrydate'] = dt.datetime.now()\n",
    "\n",
    "df.insert(0,'Id',uuid.uuid4()) \n",
    "df.Id= df.Id.apply(lambda x: uuid.uuid4()) # adding unique identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 10 columns):\n",
      "Id                                    50 non-null object\n",
      "Newspaper                             50 non-null object\n",
      "state                                 50 non-null object\n",
      "DailyCirculation_2004                 50 non-null object\n",
      "DailyCirculation_2013                 50 non-null object\n",
      "ChangeInDailyCirculation_2004_2013    50 non-null object\n",
      "WinnersAndFinalists_1990_2003         50 non-null int64\n",
      "WinnersAndFinalists_2004_2014         50 non-null int64\n",
      "WinnersAndFinalists_1990_2014         50 non-null int64\n",
      "Entrydate                             50 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(3), object(6)\n",
      "memory usage: 4.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() # checking the data frame structure to make type change if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>state</th>\n",
       "      <th>DailyCirculation_2004</th>\n",
       "      <th>DailyCirculation_2013</th>\n",
       "      <th>ChangeInDailyCirculation_2004_2013</th>\n",
       "      <th>WinnersAndFinalists_1990_2003</th>\n",
       "      <th>WinnersAndFinalists_2004_2014</th>\n",
       "      <th>WinnersAndFinalists_1990_2014</th>\n",
       "      <th>Entrydate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>c04ef147-c92a-4f36-a461-78f60a3030d3</td>\n",
       "      <td>Cleveland Plain Dealer</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>367,528</td>\n",
       "      <td>311,605</td>\n",
       "      <td>-15%</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>2017-11-18 15:48:36.202228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>57a414ac-1d58-4641-bc75-a1b59fe43c86</td>\n",
       "      <td>Louisville Courier-Journal</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>216,934</td>\n",
       "      <td>131,208</td>\n",
       "      <td>-40%</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-11-18 15:48:36.202228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ca563ebc-8d9a-4a84-b829-e308b5677066</td>\n",
       "      <td>Oregonian</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>339,169</td>\n",
       "      <td>228,909</td>\n",
       "      <td>-33%</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>2017-11-18 15:48:36.202228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Id                   Newspaper  \\\n",
       "21  c04ef147-c92a-4f36-a461-78f60a3030d3      Cleveland Plain Dealer   \n",
       "48  57a414ac-1d58-4641-bc75-a1b59fe43c86  Louisville Courier-Journal   \n",
       "26  ca563ebc-8d9a-4a84-b829-e308b5677066                   Oregonian   \n",
       "\n",
       "       state DailyCirculation_2004 DailyCirculation_2013  \\\n",
       "21      Ohio               367,528               311,605   \n",
       "48  Kentucky               216,934               131,208   \n",
       "26    Oregon               339,169               228,909   \n",
       "\n",
       "   ChangeInDailyCirculation_2004_2013  WinnersAndFinalists_1990_2003  \\\n",
       "21                               -15%                              4   \n",
       "48                               -40%                              0   \n",
       "26                               -33%                              9   \n",
       "\n",
       "    WinnersAndFinalists_2004_2014  WinnersAndFinalists_1990_2014  \\\n",
       "21                              7                             11   \n",
       "48                              3                              3   \n",
       "26                              8                             17   \n",
       "\n",
       "                    Entrydate  \n",
       "21 2017-11-18 15:48:36.202228  \n",
       "48 2017-11-18 15:48:36.202228  \n",
       "26 2017-11-18 15:48:36.202228  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3) #number are stored with commas which we need to replace before stat analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1.1.2 Inserting Pulitzer Data into Cassandra\n",
    "The step is done for persistence and reliability among other benefits using a database cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the connection points details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_con=pd.read_csv('~/connection_point.csv',header=0) # this is done to make add basic level of security. \n",
    "#Please note that this file is not uploaded. It is only present in the Jupyter server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "import time\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.concurrent import execute_concurrent_with_args\n",
    "from cassandra.query import tuple_factory\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "def _insertData(params):\n",
    "    cluster = Cluster(contact_points=[df_con.ip[0]], auth_provider = \\\n",
    "                      PlainTextAuthProvider(username=df_con.user[0], \\\n",
    "                                            password=df_con.token[0]))\n",
    "    session = cluster.connect()\n",
    "    session.set_keyspace('capstone')\n",
    "    session.row_factory = tuple_factory\n",
    "    prepared=session.prepare(\"INSERT INTO capstone.pulitzer \\\n",
    "                             (id,Newspaper,state,DailyCirculation_2004,DailyCirculation_2013, \\\n",
    "                             ChangeDailyCirculation_2004_2013,WinNFinalists_1990_2003, \\\n",
    "                             WinNFinalists_2004_2014,WinNFinalists_1990_2014,Entrydate) \\\n",
    "                             VALUES (?,?,?,?,?,?,?,?,?,?)\")\n",
    "    \n",
    "    #using datastax driver for multiprocessing \n",
    "    execute_concurrent_with_args(session, prepared, params, concurrency=50) \n",
    "    return None\n",
    "\n",
    "def multiprocess(params):\n",
    "    pool = Pool(processes=4)\n",
    "    results = [pool.map(_insertData, (params[n:n+100],)) for n in range(0, len(params),100)]\n",
    "    return results\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parameters=[]\n",
    "    for index, row in enumerate(df.values):        \n",
    "        (a,b,c,d,e,f,g,h,i,j) = row\n",
    "        row1=(a,str(b),str(c),str(d),str(e),str(f),str(g),str(h),str(i),j)\n",
    "        parameters.append(row1)           \n",
    "    a = multiprocess(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Gathering US State and Country GDP\n",
    "GDP data is one file per state or country for example \"usgs_1957_2015-1.csv\", this file holds NJ specific GDP starting from 1957 to 2015. I have downloaded 50 + 1 data files from the site and stored it in a local data folder. There are 50(states) + 1 for US files in the data directory. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2.1 Read and Prepare GDP Data\n",
    "\n",
    "Step 1: Loading 51 files in a list for bulk processing. <br>\n",
    "Step 2: Handling file format issues while reading i.e. skipping few top lines, bad lines(engine = python) and selecting the required columns. <br>\n",
    "Step 4: Changing the columns to sync with the Pulitzer data frame.<br>\n",
    "Step 3: Merging the data frame vertically. Merging may throw few warnings which can be ignored. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "files = [file for file in os.listdir( './Data' ) \\\n",
    "         if file.startswith(\"usgs_1957_2015\")]\n",
    "gdp_merged=pd.DataFrame()\n",
    "\n",
    "for file_ in files:\n",
    "    filename='./Data/'+file_\n",
    "    df = pd.read_csv(filename,skiprows=0,header=1,skipfooter=6,quoting=3,error_bad_lines=False, engine='python', usecols=range(0,3))\n",
    "    df['State']=df.columns[2].split('-')[1][:2]\n",
    "    del df[df.columns[2]]\n",
    "    df.columns=['year','GDP-billion','state']\n",
    "    gdp_merged=pd.concat([gdp_merged,df],ignore_index=1,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Adding basic data audit fields just in case we run into data conflicts in later phases of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tz = timezone('EST') # adding time zone info\n",
    "datetime.now(tz) \n",
    "gdp_merged['Entrydate'] = dt.datetime.now()\n",
    "\n",
    "gdp_merged.insert(0,'Id',uuid.uuid4()) \n",
    "gdp_merged.Id= gdp_merged.Id.apply(lambda x: uuid.uuid4()) # adding unique identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1.2.2 Inserting GDP data to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _insertData(params):\n",
    "    cluster = Cluster(contact_points=[df_con.ip[0]], auth_provider = \\\n",
    "                      PlainTextAuthProvider(username=df_con.user[0], \\\n",
    "                                            password=df_con.token[0]))\n",
    "    session = cluster.connect()\n",
    "    session.set_keyspace('capstone')\n",
    "    session.row_factory = tuple_factory\n",
    "    prepared=session.prepare(\"INSERT INTO capstone.GDP(id,year,GDP,state,entrydate) VALUES (?,?,?,?,?)\")\n",
    "    \n",
    "    #using datastax driver for multiprocessing \n",
    "    execute_concurrent_with_args(session, prepared, params, concurrency=50) \n",
    "    return None\n",
    "\n",
    "def multiprocess(params):\n",
    "    pool = Pool(processes=4)\n",
    "    results = [pool.map(_insertData, (params[n:n+100],)) for n in range(0, len(params),100)]\n",
    "    return results\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parameters=[]\n",
    "    for index, row in enumerate(gdp_merged.values):        \n",
    "        (a,b,c,d,e) = row\n",
    "        row1=(a,str(b),int(c),str(d),i)\n",
    "        parameters.append(row1)           \n",
    "    a = multiprocess(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Gathering Crime Data\n",
    "Crime data are collected from two different sources thus we need to merge it at the end. I have only considered crime index and violent crime for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3.1 Preparing Crime Index Data\n",
    "Step 1: Accessing GCP bucket for the crime excel files. <br>\n",
    "Step 2: Adding audit fields. <br>\n",
    "Step 3: Spliting dataframe column to get year and state. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blob = storage.Blob('CrimeIndex.xlsx',bucket)\n",
    "with open('Crimeindex.xlsx', 'wb') as file_obj:\n",
    "    blob.download_to_file(file_obj)\n",
    "    \n",
    "xl=pd.ExcelFile('Crimeindex.xlsx')\n",
    "df_crime=xl.parse('Sheet1',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tz = timezone('EST') # adding time zone info\n",
    "datetime.now(tz) \n",
    "df_crime['Entrydate'] = dt.datetime.now()\n",
    "\n",
    "df_crime.insert(0,'Id',uuid.uuid4()) \n",
    "df_crime.Id= df_crime.Id.apply(lambda x: uuid.uuid4()) # adding unique identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crime.columns=['Id','rank','crimeindex','state-population','Entrydate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crime.insert(4,'state','')\n",
    "df_crime['state']=df_crime['state-population'].apply(lambda x: x.split('/')[0][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crime.insert(5,'population','')\n",
    "df_crime['population']=df_crime['state-population'].apply(lambda x: x.split('/')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_crime[df_crime.columns[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_crime.sample(3) #['state-population'].values[0].split('/')[1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3.2 Inserting Crime Index Data into Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _insertData(params):\n",
    "    cluster = Cluster(contact_points=[df_con.ip[0]], auth_provider = \\\n",
    "                      PlainTextAuthProvider(username=df_con.user[0], \\\n",
    "                                            password=df_con.token[0]))\n",
    "    session = cluster.connect()\n",
    "    session.set_keyspace('capstone')\n",
    "    session.row_factory = tuple_factory\n",
    "    prepared=session.prepare(\"INSERT INTO capstone.crimeindex(id,rank,\\\n",
    "                              crimeindex,state,population,entrydate)\\\n",
    "                              VALUES (?,?,?,?,?,?)\")\n",
    "    \n",
    "    #using datastax driver for multiprocessing \n",
    "    execute_concurrent_with_args(session, prepared, params, concurrency=50) \n",
    "    return None\n",
    "\n",
    "def multiprocess(params):\n",
    "    pool = Pool(processes=4)\n",
    "    results = [pool.map(_insertData, (params[n:n+100],)) for n in range(0, len(params),100)]\n",
    "    return results\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parameters=[]\n",
    "    for index, row in enumerate(df_crime.values):        \n",
    "        (a,b,c,d,e,f) = row\n",
    "        row1=(a,int(b),int(c),str(d),int(e.replace(',','')),f)\n",
    "        parameters.append(row1)           \n",
    "    a = multiprocess(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Gathering Crime by US State and US\n",
    "Crime tables are downloaded and uploaded to GCP for easy access. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4.1 Preparing Crime by US State and US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Step 1: Accessing Crime Tables excel file from GCP <br>\n",
    "Step 2: Parsing it - skiping few top rows and bad lines <br>\n",
    "Step 3: Processing the year as it is merged into one data <br>\n",
    "Step 4: Changing the Data frame columns to sync with Pulitzer <br>\n",
    "Step 5: Adding Audit fields. <br>\n",
    "Step 6: Inserting data into Cassandra tables. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blob = storage.Blob('table_1_crime_in_the_united_states_by_volume_and_rate_per_100000_inhabitants_1995-2014.xls',bucket)\n",
    "with open('table_1_crime_in_the_united_states_by_volume_and_rate_per_100000_inhabitants_1995-2014.xls', 'wb') as file_obj:\n",
    "    blob.download_to_file(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename='table_1_crime_in_the_united_states_by_volume_and_rate_per_100000_inhabitants_1995-2014.xls'\n",
    "xl=pd.ExcelFile(filename)\n",
    "df_crime_byvol=xl.parse('14tbl01',skiprows=3,header=0,skipfooter=10,error_bad_lines=False, engine='python', usecols=range(0,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crime_byvol['Year']=df_crime_byvol.Year.apply(lambda x: int(str(x)[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crime_byvol.columns=['year','population','violentcrime','violentcrimerate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tz = timezone('EST') # adding time zone info\n",
    "datetime.now(tz) \n",
    "df_crime_byvol['Entrydate'] = dt.datetime.now()\n",
    "\n",
    "df_crime_byvol.insert(0,'Id',uuid.uuid4()) \n",
    "df_crime_byvol.Id= df_crime.Id.apply(lambda x: uuid.uuid4()) # adding unique identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>year</th>\n",
       "      <th>population</th>\n",
       "      <th>violentcrime</th>\n",
       "      <th>violentcrimerate</th>\n",
       "      <th>Entrydate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>921006ea-e4ad-412f-b0c8-9a494feb08fb</td>\n",
       "      <td>1995</td>\n",
       "      <td>262803276</td>\n",
       "      <td>1798792</td>\n",
       "      <td>684.5</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3cda340b-794f-49da-9b73-82fdd189c67d</td>\n",
       "      <td>1996</td>\n",
       "      <td>265228572</td>\n",
       "      <td>1688540</td>\n",
       "      <td>636.6</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8d58a704-0aa5-46a8-9868-e581592bffc1</td>\n",
       "      <td>1997</td>\n",
       "      <td>267783607</td>\n",
       "      <td>1636096</td>\n",
       "      <td>611.0</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bfac0bd3-8fc6-4cf0-b5bd-16a9a35117fa</td>\n",
       "      <td>1998</td>\n",
       "      <td>270248003</td>\n",
       "      <td>1533887</td>\n",
       "      <td>567.6</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80be7628-bd41-44c6-87fb-310667d546cc</td>\n",
       "      <td>1999</td>\n",
       "      <td>272690813</td>\n",
       "      <td>1426044</td>\n",
       "      <td>523.0</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69f9c891-aa66-40e3-81d7-b2cd728418be</td>\n",
       "      <td>2000</td>\n",
       "      <td>281421906</td>\n",
       "      <td>1425486</td>\n",
       "      <td>506.5</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>e1fd6370-291f-44e4-b742-5b34689a5d87</td>\n",
       "      <td>2001</td>\n",
       "      <td>285317559</td>\n",
       "      <td>1439480</td>\n",
       "      <td>504.5</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c5030888-1574-49e9-baea-48410b12a52d</td>\n",
       "      <td>2002</td>\n",
       "      <td>287973924</td>\n",
       "      <td>1423677</td>\n",
       "      <td>494.4</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>edce9436-225c-4d11-99b9-ca8d85bbfcfc</td>\n",
       "      <td>2003</td>\n",
       "      <td>290788976</td>\n",
       "      <td>1383676</td>\n",
       "      <td>475.8</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1b4b77c0-7642-4125-869a-d2e6b5ba52a9</td>\n",
       "      <td>2004</td>\n",
       "      <td>293656842</td>\n",
       "      <td>1360088</td>\n",
       "      <td>463.2</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23cab8f6-541f-43a2-8148-3b2061c3a8ca</td>\n",
       "      <td>2005</td>\n",
       "      <td>296507061</td>\n",
       "      <td>1390745</td>\n",
       "      <td>469.0</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5d454fec-8269-46dd-b6c7-ff558c047077</td>\n",
       "      <td>2006</td>\n",
       "      <td>299398484</td>\n",
       "      <td>1435123</td>\n",
       "      <td>479.3</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>a8a66472-0636-40d8-80b7-d270535f2a6a</td>\n",
       "      <td>2007</td>\n",
       "      <td>301621157</td>\n",
       "      <td>1422970</td>\n",
       "      <td>471.8</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ef69219d-5231-4bb9-9219-933d2155785b</td>\n",
       "      <td>2008</td>\n",
       "      <td>304059724</td>\n",
       "      <td>1394461</td>\n",
       "      <td>458.6</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>88f43e24-98e1-4426-aa9b-2cba483443cd</td>\n",
       "      <td>2009</td>\n",
       "      <td>307006550</td>\n",
       "      <td>1325896</td>\n",
       "      <td>431.9</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>73eb5e58-f55b-4119-8140-9ce38ed3719d</td>\n",
       "      <td>2010</td>\n",
       "      <td>309330219</td>\n",
       "      <td>1251248</td>\n",
       "      <td>404.5</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0f01b4b0-397c-4e98-8db3-4170cd614235</td>\n",
       "      <td>2011</td>\n",
       "      <td>311587816</td>\n",
       "      <td>1206005</td>\n",
       "      <td>387.1</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>e6bada12-2cc4-4703-be94-f21771b1bc6e</td>\n",
       "      <td>2012</td>\n",
       "      <td>313873685</td>\n",
       "      <td>1217057</td>\n",
       "      <td>387.8</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>063ae60b-055e-4042-93f9-f285da8f5026</td>\n",
       "      <td>2013</td>\n",
       "      <td>316497531</td>\n",
       "      <td>1168298</td>\n",
       "      <td>369.1</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a287f2fd-2c5b-4075-8385-aae7220e5928</td>\n",
       "      <td>2014</td>\n",
       "      <td>318857056</td>\n",
       "      <td>1165383</td>\n",
       "      <td>365.5</td>\n",
       "      <td>2017-10-23 00:27:37.706506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Id  year  population  violentcrime  \\\n",
       "0   921006ea-e4ad-412f-b0c8-9a494feb08fb  1995   262803276       1798792   \n",
       "1   3cda340b-794f-49da-9b73-82fdd189c67d  1996   265228572       1688540   \n",
       "2   8d58a704-0aa5-46a8-9868-e581592bffc1  1997   267783607       1636096   \n",
       "3   bfac0bd3-8fc6-4cf0-b5bd-16a9a35117fa  1998   270248003       1533887   \n",
       "4   80be7628-bd41-44c6-87fb-310667d546cc  1999   272690813       1426044   \n",
       "5   69f9c891-aa66-40e3-81d7-b2cd728418be  2000   281421906       1425486   \n",
       "6   e1fd6370-291f-44e4-b742-5b34689a5d87  2001   285317559       1439480   \n",
       "7   c5030888-1574-49e9-baea-48410b12a52d  2002   287973924       1423677   \n",
       "8   edce9436-225c-4d11-99b9-ca8d85bbfcfc  2003   290788976       1383676   \n",
       "9   1b4b77c0-7642-4125-869a-d2e6b5ba52a9  2004   293656842       1360088   \n",
       "10  23cab8f6-541f-43a2-8148-3b2061c3a8ca  2005   296507061       1390745   \n",
       "11  5d454fec-8269-46dd-b6c7-ff558c047077  2006   299398484       1435123   \n",
       "12  a8a66472-0636-40d8-80b7-d270535f2a6a  2007   301621157       1422970   \n",
       "13  ef69219d-5231-4bb9-9219-933d2155785b  2008   304059724       1394461   \n",
       "14  88f43e24-98e1-4426-aa9b-2cba483443cd  2009   307006550       1325896   \n",
       "15  73eb5e58-f55b-4119-8140-9ce38ed3719d  2010   309330219       1251248   \n",
       "16  0f01b4b0-397c-4e98-8db3-4170cd614235  2011   311587816       1206005   \n",
       "17  e6bada12-2cc4-4703-be94-f21771b1bc6e  2012   313873685       1217057   \n",
       "18  063ae60b-055e-4042-93f9-f285da8f5026  2013   316497531       1168298   \n",
       "19  a287f2fd-2c5b-4075-8385-aae7220e5928  2014   318857056       1165383   \n",
       "\n",
       "    violentcrimerate                  Entrydate  \n",
       "0              684.5 2017-10-23 00:27:37.706506  \n",
       "1              636.6 2017-10-23 00:27:37.706506  \n",
       "2              611.0 2017-10-23 00:27:37.706506  \n",
       "3              567.6 2017-10-23 00:27:37.706506  \n",
       "4              523.0 2017-10-23 00:27:37.706506  \n",
       "5              506.5 2017-10-23 00:27:37.706506  \n",
       "6              504.5 2017-10-23 00:27:37.706506  \n",
       "7              494.4 2017-10-23 00:27:37.706506  \n",
       "8              475.8 2017-10-23 00:27:37.706506  \n",
       "9              463.2 2017-10-23 00:27:37.706506  \n",
       "10             469.0 2017-10-23 00:27:37.706506  \n",
       "11             479.3 2017-10-23 00:27:37.706506  \n",
       "12             471.8 2017-10-23 00:27:37.706506  \n",
       "13             458.6 2017-10-23 00:27:37.706506  \n",
       "14             431.9 2017-10-23 00:27:37.706506  \n",
       "15             404.5 2017-10-23 00:27:37.706506  \n",
       "16             387.1 2017-10-23 00:27:37.706506  \n",
       "17             387.8 2017-10-23 00:27:37.706506  \n",
       "18             369.1 2017-10-23 00:27:37.706506  \n",
       "19             365.5 2017-10-23 00:27:37.706506  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crime_byvol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4.2 Inserting Crime by Volume into Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _insertData(params):\n",
    "    cluster = Cluster(contact_points=[df_con.ip[0]], auth_provider = \\\n",
    "                      PlainTextAuthProvider(username=df_con.user[0], \\\n",
    "                                            password=df_con.token[0]))\n",
    "    session = cluster.connect()\n",
    "    session.set_keyspace('capstone')\n",
    "    session.row_factory = tuple_factory\n",
    "    prepared=session.prepare(\"INSERT INTO capstone.crimebyvol(id,year,population, \\\n",
    "                              violentcrime,violentcrimerate,entrydate) \\\n",
    "                              VALUES (?,?,?,?,?,?)\")\n",
    "    \n",
    "    #using datastax driver for multiprocessing \n",
    "    execute_concurrent_with_args(session, prepared, params, concurrency=50) \n",
    "    return None\n",
    "\n",
    "def multiprocess(params):\n",
    "    pool = Pool(processes=4)\n",
    "    results = [pool.map(_insertData, (params[n:n+100],)) for n in range(0, len(params),100)]\n",
    "    return results\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parameters=[]\n",
    "    for index, row in enumerate(df_crime_byvol.values):        \n",
    "        (a,b,c,d,e,f) = row\n",
    "        row1=(a,str(b),int(c),int(d),int(e),f)\n",
    "        parameters.append(row1)           \n",
    "    a = multiprocess(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 Gathering Population Data 2000 to 2016\n",
    "Population data is collected over two censuses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5.1 Preparing Population Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Reading population from 2000 - 2009 data files <br>\n",
    "Step 2: Selecting the required fields <br>\n",
    "Step 3: Filtering data to make it demographic agnostic <br> \n",
    "Step 4: Dropping not needed data fields <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename='./Data/st-est00int-alldata.csv'\n",
    "df_=pd.read_csv(filename,header=0,usecols=[2,3,4,5,6,7,9,10,11,12,13,14,15,16,17,18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['STATE', 'NAME', 'SEX', 'ORIGIN', 'RACE', 'AGEGRP', 'POPESTIMATE2000',\n",
       "       'POPESTIMATE2001', 'POPESTIMATE2002', 'POPESTIMATE2003',\n",
       "       'POPESTIMATE2004', 'POPESTIMATE2005', 'POPESTIMATE2006',\n",
       "       'POPESTIMATE2007', 'POPESTIMATE2008', 'POPESTIMATE2009'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is filtering the data frame to remove data related to demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pop=df_[(df_.SEX==0) & (df_.ORIGIN==0) & (df_.RACE==0) & (df_.AGEGRP==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping not needed fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_=df_pop.drop(['SEX', 'ORIGIN', 'RACE', 'AGEGRP'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['STATE', 'NAME', 'POPESTIMATE2000', 'POPESTIMATE2001',\n",
       "       'POPESTIMATE2002', 'POPESTIMATE2003', 'POPESTIMATE2004',\n",
       "       'POPESTIMATE2005', 'POPESTIMATE2006', 'POPESTIMATE2007',\n",
       "       'POPESTIMATE2008', 'POPESTIMATE2009'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.columns # Please make a note of the estimated figures for 2000 to 2009 based on state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading population data 2010 to 2016 \n",
    "\n",
    "Step 1: Reading the 2000 census data \n",
    "Step 2: Selecting the fields matching the 2000 to 2009 population dataframe\n",
    "Step 3: We have to merge the dataframe to get the consolidated population dataframe\n",
    "Step 4: Drop the unwanted fields\n",
    "Step 5: Adding Audit fields\n",
    "Step 6: Inserting data into Cassandra tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename='./Data/nst-est2016-alldata.csv'\n",
    "df_1=pd.read_csv(filename,header=0,usecols=[1,3,4,7,8,9,10,11,12,13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['REGION', 'STATE', 'NAME', 'POPESTIMATE2010', 'POPESTIMATE2011',\n",
       "       'POPESTIMATE2012', 'POPESTIMATE2013', 'POPESTIMATE2014',\n",
       "       'POPESTIMATE2015', 'POPESTIMATE2016'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.columns #Please make a note of the fields. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the dataframes to create a unified data frame for population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pop_merged=pd.merge(df_1,df_,how='left', on='NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the merged dataframe for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['REGION', 'STATE_x', 'NAME', 'POPESTIMATE2010', 'POPESTIMATE2011',\n",
       "       'POPESTIMATE2012', 'POPESTIMATE2013', 'POPESTIMATE2014',\n",
       "       'POPESTIMATE2015', 'POPESTIMATE2016', 'STATE_y', 'POPESTIMATE2000',\n",
       "       'POPESTIMATE2001', 'POPESTIMATE2002', 'POPESTIMATE2003',\n",
       "       'POPESTIMATE2004', 'POPESTIMATE2005', 'POPESTIMATE2006',\n",
       "       'POPESTIMATE2007', 'POPESTIMATE2008', 'POPESTIMATE2009'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pop_merged.columns#sample(15,axis=1)#.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the merge columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pop_merged_=df_pop_merged.drop(['STATE_x', 'STATE_y'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Adding audit fileds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tz = timezone('EST') # adding time zone info\n",
    "datetime.now(tz) \n",
    "df_pop_merged_['Entrydate'] = dt.datetime.now()\n",
    "\n",
    "df_pop_merged_.insert(0,'Id',uuid.uuid4()) \n",
    "df_pop_merged_.Id= df_pop_merged_.Id.apply(lambda x: uuid.uuid4()) # adding unique identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_pop_merged_.sample(3)#.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1.5.2 Inserting Population Data to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "import time\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.concurrent import execute_concurrent_with_args\n",
    "from cassandra.query import tuple_factory\n",
    "from cassandra.auth import PlainTextAuthProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _insertData(params):\n",
    "    cluster = Cluster(contact_points=[df_con.ip[0]], auth_provider = \\\n",
    "                      PlainTextAuthProvider(username=df_con.user[0], \\\n",
    "                                            password=df_con.token[0]))\n",
    "    session = cluster.connect()\n",
    "    session.set_keyspace('capstone')\n",
    "    session.row_factory = tuple_factory\n",
    "    prepared=session.prepare(\"INSERT INTO capstone.population(id,region,state,   \\\n",
    "                              POPESTIMATE2000, POPESTIMATE2001, POPESTIMATE2002, \\\n",
    "                              POPESTIMATE2003, POPESTIMATE2004, POPESTIMATE2005, \\\n",
    "                              POPESTIMATE2006, POPESTIMATE2007, POPESTIMATE2008, \\\n",
    "                              POPESTIMATE2009, POPESTIMATE2010, POPESTIMATE2011, \\\n",
    "                              POPESTIMATE2012, POPESTIMATE2013, POPESTIMATE2014, \\\n",
    "                              POPESTIMATE2015, POPESTIMATE2016,entrydate)                  \\\n",
    "                              VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\")\n",
    "    \n",
    "    #using datastax driver for multiprocessing \n",
    "    execute_concurrent_with_args(session, prepared, params, concurrency=50) \n",
    "    return None\n",
    "\n",
    "def multiprocess(params):\n",
    "    pool = Pool(processes=4)\n",
    "    results = [pool.map(_insertData, (params[n:n+100],)) for n in range(0, len(params),100)]\n",
    "    return results\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parameters=[]\n",
    "    for index, row in enumerate(df_pop_merged_.values):        \n",
    "        (a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u) = row\n",
    "        row1=(a,str(b),c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u)\n",
    "        parameters.append(row1)           \n",
    "    a = multiprocess(parameters)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4.2 Reading data from Cassandra Tables - Phase - II\n",
    "### The next stage of the project is to read data from Cassandra tables and logically group them for analysis\n",
    "\n",
    "This section is not optimized wrt code for example we can writena function and call it for Cassandra data access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "import time\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.concurrent import execute_concurrent_with_args\n",
    "from cassandra.query import tuple_factory\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_con=pd.read_csv('~/connection_point.csv',header=0)  # This is done to add basic level of security\n",
    "df_code=pd.read_csv('~/states_code.txt',header=0,sep='\\t') # this is dict will serve state or two digit mapping lookup\n",
    "states_dict=dict(zip(df_code.Abbreviation,df_code.State)) # creation of dict. This will be used later in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "def pandas_factory(colnames, rows):\n",
    "    return pd.DataFrame(rows, columns=colnames)\n",
    "\n",
    "def _fetchData(query_):\n",
    "    cluster = Cluster(contact_points=[df_con.ip[0]], auth_provider = \\\n",
    "                      PlainTextAuthProvider(username=df_con.user[0], \\\n",
    "                                            password=df_con.token[0]))\n",
    "    session = cluster.connect()\n",
    "    session.set_keyspace('capstone')\n",
    "    session.row_factory = pandas_factory    \n",
    "    rows = session.execute(query_)\n",
    "    return rows\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query_pram=\"SELECT  Newspaper,state,DailyCirculation_2004, \\\n",
    "                DailyCirculation_2013,ChangeDailyCirculation_2004_2013,\\\n",
    "                WinNFinalists_1990_2003,WinNFinalists_2004_2014,WinNFinalists_1990_2014\\\n",
    "                FROM capstone.pulitzer\"\n",
    "    rows = _fetchData(query_pram)\n",
    "    df_pulitzer=rows._current_rows\n",
    "    #print(display(df_pulitzer))\n",
    "    \n",
    "    #\n",
    "    query_pram=\"SELECT  gdp,state,year FROM capstone.gdp\"\n",
    "    rows = _fetchData(query_pram)\n",
    "    df_gdp=rows._current_rows\n",
    "    #print(display(df_gdp))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_gdp.state=df_gdp.state.apply(lambda x: states_dict[x]) #Using the state dict to convert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pulitzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Adding 2004 GDP Data to the Pulitzer\n",
    "Pulitzer only have data related to 2004,2013,2014. So we have to filter all data related to 2004,2014 and 2014 from GDP and merge it to Pulitzer.\n",
    "\n",
    "bfd n\n",
    "Step 1: Adding 2004 Data to Pulitzer <br>\n",
    "Step 2: Adding 2013 Data to Pulitzer <br>\n",
    "Step 3: Adding 2014 Data to Pulitzer <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_gdp_sorted=df_gdp[df_gdp.year=='2004'] #sorting the data frame to select\n",
    "df_gdp_sorted.columns=['GDP_2004','state','year_2004'] #changing columns to sync with Pulitzer\n",
    "df__=pd.merge(df_pulitzer,df_gdp_sorted.drop_duplicates(),how='left', on='state') \n",
    "# merging the data frames to add GDP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df__.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding 2013 GDP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_gdp_sorted=df_gdp[df_gdp.year=='2013']\n",
    "df_gdp_sorted.columns=['GDP_2013','state','year_2013']\n",
    "df__=pd.merge(df__,df_gdp_sorted.drop_duplicates(),how='left', on='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df__.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding 2014 GDP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_gdp_sorted=df_gdp[df_gdp.year=='2014']\n",
    "df_gdp_sorted.columns=['GDP_2014','state','year_2014']\n",
    "df__=pd.merge(df__,df_gdp_sorted.drop_duplicates(),how='left', on='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Adding Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "def pandas_factory(colnames, rows):\n",
    "    return pd.DataFrame(rows, columns=colnames)\n",
    "\n",
    "def _fetchData(query_):\n",
    "    cluster = Cluster(contact_points=[df_con.ip[0]], auth_provider = \\\n",
    "                      PlainTextAuthProvider(username=df_con.user[0], \\\n",
    "                                            password=df_con.token[0]))\n",
    "    session = cluster.connect()\n",
    "    session.set_keyspace('capstone')\n",
    "    session.row_factory = pandas_factory    \n",
    "    rows = session.execute(query_)\n",
    "    return rows\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query_pram=\"SELECT  crimeindex,state FROM capstone.crimeindex\"\n",
    "    rows = _fetchData(query_pram)\n",
    "    df_crimeindex=rows._current_rows\n",
    "    #print(display(df_pulitzer))\n",
    "    \n",
    "    #\n",
    "    query_pram=\"SELECT  violentcrime,year FROM capstone.crimebyvol\"\n",
    "    rows = _fetchData(query_pram)\n",
    "    df_crimebyvol=rows._current_rows\n",
    "    #print(display(df_gdp))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_crimeindex\n",
    "df__=pd.merge(df__,df_crimeindex,how='left', on='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df__.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding violentcrime data for 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crimebyvol_sorted=df_crimebyvol[df_crimebyvol.year=='2004']\n",
    "#df_crimebyvol_sorted.insert(2,'state','US')\n",
    "df_crimebyvol_sorted.columns=['violentcrime_2004','year_2004']\n",
    "df__=pd.merge(df__,df_crimebyvol_sorted,how='left', on='year_2004')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df__.sample(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding violent crime data for 2013 to Pulitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crimebyvol_sorted=df_crimebyvol[df_crimebyvol.year=='2013']\n",
    "df_crimebyvol_sorted.columns=['violentcrime_2013','year_2013']\n",
    "df__=pd.merge(df__,df_crimebyvol_sorted,how='left', on='year_2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df__.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Adding violent crime for 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crimebyvol_sorted=df_crimebyvol[df_crimebyvol.year=='2014']\n",
    "df_crimebyvol_sorted.columns=['violentcrime_2014','year_2014']\n",
    "df__=pd.merge(df__,df_crimebyvol_sorted,how='left', on='year_2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df__#.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Adding Population Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "def pandas_factory(colnames, rows):\n",
    "    return pd.DataFrame(rows, columns=colnames)\n",
    "\n",
    "def _fetchData(query_):\n",
    "    cluster = Cluster(contact_points=[df_con.ip[0]], auth_provider = \\\n",
    "                      PlainTextAuthProvider(username=df_con.user[0], \\\n",
    "                                            password=df_con.token[0]))\n",
    "    session = cluster.connect()\n",
    "    session.set_keyspace('capstone')\n",
    "    session.row_factory = pandas_factory    \n",
    "    rows = session.execute(query_)\n",
    "    return rows\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query_pram=\"SELECT  * FROM capstone.population\"\n",
    "    rows = _fetchData(query_pram)\n",
    "    df_popu=rows._current_rows\n",
    "    #print(display(df_pulitzer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_popu_sorted=df_popu[['popestimate2004','popestimate2013','popestimate2014','state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df__=pd.merge(df__,df_popu_sorted,how='left', on='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df__.sample(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['newspaper', 'state', 'dailycirculation_2004', 'dailycirculation_2013',\n",
       "       'changedailycirculation_2004_2013', 'winnfinalists_1990_2003',\n",
       "       'winnfinalists_2004_2014', 'winnfinalists_1990_2014', 'GDP_2004',\n",
       "       'year_2004', 'GDP_2013', 'year_2013', 'GDP_2014', 'year_2014',\n",
       "       'crimeindex', 'violentcrime_2004', 'violentcrime_2013',\n",
       "       'violentcrime_2014', 'popestimate2004', 'popestimate2013',\n",
       "       'popestimate2014'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df__.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df__=df__.drop(['year_2004', 'year_2013', 'year_2014'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Adding Audit Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tz = timezone('EST') # adding time zone info\n",
    "datetime.now(tz) \n",
    "df__['Entrydate'] = dt.datetime.now()\n",
    "\n",
    "df__.insert(0,'Id',uuid.uuid4()) \n",
    "df__.Id= df__.Id.apply(lambda x: uuid.uuid4()) # adding unique identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'newspaper', 'state', 'dailycirculation_2004',\n",
       "       'dailycirculation_2013', 'changedailycirculation_2004_2013',\n",
       "       'winnfinalists_1990_2003', 'winnfinalists_2004_2014',\n",
       "       'winnfinalists_1990_2014', 'GDP_2004', 'GDP_2013', 'GDP_2014',\n",
       "       'crimeindex', 'violentcrime_2004', 'violentcrime_2013',\n",
       "       'violentcrime_2014', 'popestimate2004', 'popestimate2013',\n",
       "       'popestimate2014', 'Entrydate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df__.to_csv('pulitzerFinal.csv') # This step is just for backup. We have to insert the merged data to Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Inserting Pulitzer Final Data into Cassandra\n",
    "\n",
    "This step concludes our Data Gathering and Preparations. Now, we move to Data Discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _insertData(params):\n",
    "    cluster = Cluster(contact_points=[df_con.ip[0]], auth_provider = \\\n",
    "                      PlainTextAuthProvider(username=df_con.user[0], \\\n",
    "                                            password=df_con.token[0]))\n",
    "    session = cluster.connect()\n",
    "    session.set_keyspace('capstone')\n",
    "    session.row_factory = tuple_factory\n",
    "    prepared=session.prepare(\"INSERT INTO capstone.pulitzerfinal(id,newspaper,state, \\\n",
    "                              dailycirculation_2004,dailycirculation_2013, \\\n",
    "                              changedailycirculation_2004_2013, winnfinalists_1990_2003, \\\n",
    "                              winnfinalists_2004_2014,winnfinalists_1990_2014,GDP_2004, \\\n",
    "                              GDP_2013, GDP_2014, crimeindex, violentcrime_2004,violentcrime_2013, \\\n",
    "                              violentcrime_2014, popestimate2004, popestimate2013, \\\n",
    "                              popestimate2014, entrydate) \\\n",
    "                              VALUES (?,?,?, \\\n",
    "                                      ?,?, \\\n",
    "                                      ?,?, \\\n",
    "                                      ?,?,?, \\\n",
    "                                      ?,?,?,?,?, \\\n",
    "                                      ?,?,?, \\\n",
    "                                      ?,?)\")\n",
    "\n",
    "    #using datastax driver for multiprocessing \n",
    "    execute_concurrent_with_args(session, prepared, params, concurrency=50) \n",
    "    return None\n",
    "\n",
    "def multiprocess(params):\n",
    "    pool = Pool(processes=4)\n",
    "    results = [pool.map(_insertData, (params[n:n+10],)) for n in range(0, len(params),10)]\n",
    "    return results\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parameters=[]\n",
    "    for index, row in enumerate(df__.values):        \n",
    "        (a1,a2,a3,a4,a5,a6,a7,a8,a9,a10,a11,a12,a13,a14,a15,a16,a17,a18,a19,a20) = row\n",
    "        row1=(a1,str(a2),str(a3),str(a4),str(a5),str(a6),str(a7),str(a8),str(a9), \\\n",
    "              str(a10),str(a11),str(a12),str(a13),str(a14),str(a15),str(a16),str(a17),str(a18),str(a19),a20)\n",
    "        parameters.append(row1)           \n",
    "    a = multiprocess(parameters)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
